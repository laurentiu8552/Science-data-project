### JYSK case competition

```{r Libraries necessary}
library(tidyverse)
library(caret)
library(caTools)
library(DataExplorer)
library(ggplot2)  
library(visdat)   
library(recipes)  
library(rsample)
library(forecast)
library(MASS)
library(rfm)
library(readxl)
library(dplyr)

library(ggplot2)
    library(DataExplorer)
    library(GGally)
    library(BTYD)
    library(plyr)
    library(ggplot2)
    library(lubridate)
    library(reshape2)
    library(BTYDplus)
    library(ggpubr)
    library(Hmisc)
    library(tibble)
    library(Metrics)
    library(magrittr)
    library(dplyr)
    library(car)
    library(rfm)
    library(kableExtra)
    library(reshape2)
    library(tidyverse)
    library(knitr)
    library(flexclust)
    library(clue)
    library(openxlsx)

library(tidygeocoder)
library(sf)
```

## Data pre-processing

Our data handling process had the main focus of preserving and keeping as much data as possible. For a company like JYSK, we believe this is a reasonable approach. More data likely means better models and better understanding. Such a big company has this immense amount of data available, so we should utilize it. Our process includes:

-   Deleting duplicates based on reasoning and reconciliation seen in the comments

-   Filtering to only include revenue \> 0 to exclude irrelevant (or hardly identifiable) rows that are likely returned products or discounts

-   Filtering to exclude Result rows that summarize revenue for that given day

-   Imputing empty ZIP code cells because we do not want NA's but also do not want missing data regarding the products and revenue

-   Imputing empty product categories and groups in cases where the actual product name was available, again to preserve as much info as possible

-   Imputing empty customer ID's with a placeholder instead of deleting, not to lose information on the products

-   Other smaller adjustments to have a more easily usable data set

-   Further adjustments to the data set are done in the separate use cases / models / sections

Discussion on rows possibly containing more items: As opposed to the description which assumes 1 item per order, we ran into the problem of quantities. Let's take 2 examples:

-   Product ID: 6112239 in many lines generates 63.03 (sometimes 56.73 on other days due to price changes during the year, which is an issue in itself, but one that we decided not to follow up on this time because the period is only 1 year and the changes %-wise aren't big). Then in another line the revenue is 189,07 which is exactly 3 times higher. This directly implies that we are not talking about 1 item per 1 row.

-   Same thing happens with product 6112118. The most frequent revenue (presumably price per unit) is 34,66. There is a row with revenue generated 1559,85 which is exactly 45 times higher.

These anomalies make it rather hard to generalize the data to a format where we can make sure that 1 row contains only 1 item from a certain order. One possible solution would be to take the most frequent revenue value in a product, and based on division of revenue, create a quantity column. Afterwards, x many new identical rows with the given product (and price per unit as revenue) could be generated where x is equal to quantity of the initial row. Then the row with quantity of let's say 5 would be deleted, after having distributed the revenue into 5 new rows. **With this method we could make sure that each row contains only 1 item from 1 product in a certain order.**

In this project, for our models we decided not to go with this method as it was not crucial from the recommendations standpoint, but for data clarity and further analysis, it surely would make a difference.

Another observation we had is that on specific days like November 23rd and February 1st, each and every row has it's customer id and zip codes missing. This could raise the suspicion of data collection errors from the company's side on specific days, which we thought should be noted and possibly looked into.

```{r Read the file  - data}
# Loading dataset 
data <- read_excel("jysk_case_competition_final.xlsx")
# Deleted exact duplicates because:
#   Initially, JYSK writes in their description: There are 424.965 rows split into 173.275 orders, meaning ∼2,5 items per order. This implies that the data is on item level. 1 row = 1 item. It would make sense then that some rows are identical, in cases where the customer, within the same order has purchased 2 or more of the same product. 
#   However, upon looking at the data set, we identify rows with order.id = Result. Those rows are assumed to be equal to revenue generated for that given day. We used those rows for reconciliation, and found that the numbers only match if we remove duplicates. Although we cannot be sure which duplicate lines are errors and which aren't, we decide to trust the Result rows as a starting point. There are also cases where the revenue is exactly 2,3,4,5 times higher than in other rows containing the same product. That also implies that the data set is NOT on item level. Thus, identical, duplicate rows are removed.
data <- data[!duplicated(data),]

# Rename the column 'order_value_ex_vat_ex_freight' to 'Revenue'
data <- data %>%
  dplyr::rename(revenue = order_value_ex_vat_ex_freight)
# Only include rows with revenue greater than 0 because for our analysis they do not seem relevant. Also it is very difficult to identify what they actually are. Based on values, we think they might be returns or perhaps discounts for product in certain orders, or both.
data <- data %>% 
  filter(revenue > 0)

# Previously mentioned result rows are also filtered out to avoid duplication in calculations and models where Revenue is considered
data <- data %>%
  filter(order_id != "Result")
# Imputing missing ZIP codes as well just so that 
data$customer_zip_code <- as.character(data$customer_zip_code)
data$customer_zip_code[is.na(data$customer_zip_code)] <- "9999999999"
data$customer_zip_code <- as.factor(data$customer_zip_code)

# Imputing missing product groups and categories based on item name that was available for least possible data loss during the process. Due to the complexity we use hard-coded rules that cover the most frequently appearing item names that did not have group and category assigned.

  # a, convert empty strings to NA in said columns
data <- data %>%
  mutate(
    product_group_level_1 = na_if(product_group_level_1, ""),
    product_category_level_2 = na_if(product_category_level_2, "")
  )

  # b, Define the 'product_title' vector  -------
product_title <- c(
  "Mirror NORDBORG 70×90 silver",
  "MA 160x200cm PLUS F30 DREAMZONE",
  "Sofa bed FALSLEV grey",
  "Bed frame KUNGSHAMN 160x200 light grey",
  "Bed frame KUNGSHAMN 140x200 light grey",
  "TO KARLSTAD 50x100cm yellow KR",
  "MA 90x190cm PLUS F30 DREAMZONE",
  "TM 180x200cm GOLD T30 WELLPUR",
  "MA 160x200cm GOLD F85 white/blue WELLPUR",
  "MA 160x200cm BASIC S30",
  "MA 180x200cm GOLD S110 DREAMZONE",
  "MA 140x200cm PLUS S20 DREAMZONE",
  "Dining chair PEBRINGE velvet green/black",
  "MA 90x200cm PLUS F30 DREAMZONE",
  "MA 180x200cm GOLD F115 DREAMZONE",
  "MA 140x200cm BASIC S30",
  "DCSS INGA Percale DBL grey w/piping KR",
  "Pouffe GISLEV 65x35 w/storage asphalt",
  "DCSS ANDREA DBL sand/white DBF",
  "GT KARLSTAD 40x60cm yellow KR",
  "MA 160x200cm GOLD F40 DREAMZONE",
  "Desk IKAST 42x60 black",
  "FC KARLSTAD 28x30cm light green KR",
  "MA 160x200cm BASIC S5",
  "Bed frame LIMFJORDEN 140x200 white",
  "FC KARLSTAD 28x30cm yellow KR",
  "SHE Fitted FRIDA 160x200x35 white KR",
  "MA 140x200cm GOLD F85 white/dark grey",
  "TO KARLSTAD 50x100cm light green KR",
  "Footstool ULDUM grey/black",
  "Bed frame LIMFJORDEN 160x200 white",
  "MA 140x200cm GOLD F115 DREAMZONE",
  "MA 160x200cm PLUS S55 DREAMZONE",
  "Shelving unit VANDBORG 4 s/2 d oak/black",
  "MA 90x200cm BASIC S30",
  "MA 140x200cm GOLD F40 DREAMZONE",
  "MA 80x200cm BASIC S30",
  "String curtain YXLAN 1x90x245 olive",
  "SHE Fitted FRIDA 180x200x35 white KR",
  "MA 90x200cm PLUS S20 DREAMZONE",
  "WG KARLSTAD 14x20cm yellow KR",
  "SHE Fitted FRIDA 160x200x35 beige KR",
  "MA 160x200cm GOLD F85 white/dark grey",
  "TM 120x200cm GOLD T50 grey",
  "MAAC Ø5xH23cm legs metal 5 pcs",
  "CCU SELJE 40x40x8 light blue",
  "Bed frame MANDERUP 140x200 wild oak",
  "Wardrobe EVETOFTE 143x220 white",
  "MAAC Ø5xH23cm legs metal 4 pcs",
  "MA 140x200cm PLUS S55 DREAMZONE",
  "TM 160x200cm GOLD T50 grey",
  "MA 120x200cm GOLD F30 arrows WELLPUR",
  "MA 90x200cm PLUS S55 DREAMZONE",
  "TM 140x200cm GOLD T50 grey",
  "MA 80x200cm GOLD F40 DREAMZONE",
  "SHE Fitted FRIDA 160x200x35 taupe KR",
  "SHE Fitted FRIDA 90x200x35 white KR",
  "Sofa bed MARSLEV 3 seater dark grey",
  "MA 180x200cm PLUS S20 DREAMZONE",
  "Stool VALLEKILDE Ø30 natural/white",
  "MA 180x200cm GOLD F85 white/blue WELLPUR",
  "BS KARLSTAD 100x150cm yellow KR",
  "PM KUNGSMYNTA Ø38 dark grey SDP",
  "SHE Fitted FRIDA 140x200x35 grey KR",
  "SHE Fitted FRIDA 160x200x35 grey KR",
  "CUS 200g KARITINDEN fibre 40x40cm",
  "MA 160x200cm GOLD S100 DREAMZONE",
  "Vase VILHELM Ø9xH12cm blue",
  "Pouffe AUNING 38x38 velvet grey",
  "Shelving unit JANNERUP wide 3 shel. oak",
  "MA 140x200cm GOLD F30 dots WELLPUR",
  "BT Velour LOL SURPRISE 70x140cm 2023",
  "SHE JEANETTE 240x260 blue KRONBORG",
  "MA 160x200cm GOLD F30 dots WELLPUR",
  "TM 80x200cm GOLD T50 grey",
  "SHE Fitted FRIDA 90x200x35 taupe KR",
  "SHE Fitted FRIDA 90x200x35 beige KR",
  "DCS TOVE DBL blue w/piping DBF",
  "MA 80x200cm PLUS S20 DREAMZONE",
  "SHE Fitted FRIDA 90x200x35 grey KR",
  "BT KARLSTAD 70x140cm light green KR",
  "HB 90x115cm H10 PLAIN Grey-28 DREAMZONE",
  "Nest of tables BRABRAND Ø70/50 natural/b",
  "CT 90x200cm PLUS C40 Black-07 DREAMZONE",
  "MA 140x200cm GOLD S100 DREAMZONE",
  "MA 180x200cm GOLD F40 DREAMZONE",
  "COD VEDDE 3 drawers white",
  "WG KARLSTAD 14x20cm light green KR",
  "CT 140x200cm PLUS C40 Black-07 DREAMZONE",
  "MA 120x200cm GOLD F30 dots WELLPUR",
  "MA 180x200cm GOLD F30 plain WELLPUR",
  "MA 180x200cm GOLD F30 dots WELLPUR",
  "DCS MARION SGL rose DBF KRONBORG",
  "DCS TOVE SGL blue w/piping DBF",
  "Vase PETTER Ø15xH15cm sand",
  "SHE Fitted FRIDA 180x200x35 beige KR",
  "Sofa bed OREVAD dark grey",
  "Desk SKOVLUNDE 60x120 nat. oak",
  "GT KARLSTAD 40x60cm light green KR",
  "SHE Fitted FRIDA 180x200x35 taupe KR",
  "SHE Fitted FRIDA 140x200x35 white KR",
  "TM 180x200cm GOLD T50 grey",
  "MA 180x200cm GOLD S100 DREAMZONE",
  "Bed frame PORSGRUNN 80/160x200 white",
  "Pouffe HOLEBY 40x40 w/storage black",
  "MA 80x200cm PLUS S55 DREAMZONE",
  "MA 80x200cm GOLD S100 DREAMZONE",
  "Bench BARUP velvet grey",
  "DCSS INGA Percale SGL grey w/piping KR",
  "MA 160x200cm GOLD F30 plain WELLPUR",
  "Bed frame LIMFJORDEN 90x200 white",
  "SHE JEANETTE 240x260 white KRONBORG",
  "Pouffe HOLEBY 80x40 w/storage black",
  "SHE Fitted FRIDA 180x200x35 grey KR",
  "PIL 200g FALKETIND 40x40cm",
  "MA 120x200cm GOLD F30 plain WELLPUR",
  "MA 80x200cm GOLD F30 dots WELLPUR",
  "MA 180x200cm PLUS S55 DREAMZONE",
  "Sofa bed OREVAD light grey",
  "SHE JEANETTE 240x260 taupe KRONBORG",
  "MA 90x190cm PLUS S55 DREAMZONE",
  "Bar table SANDBY 71x128 nat. oak",
  "CUR GOSSA 1x140x300 white",
  "Armchair ULDUM grey/black",
  "MA 90x200cm GOLD F30 plain WELLPUR",
  "TM 180x200cm GOLD T65 white/dark grey",
  "SHE JEANETTE 150x250 l. grey KRONBORG",
  "Decorative tray HEIMER W20xL29cm natural",
  "SHE JEANETTE 150x250 blue KRONBORG",
  "SHE JEANETTE 150x250 white KRONBORG",
  "DCSS MARY SGL coral",
  "DCS BRITT Flannel DBL sand/blue",
  "SHE JEANETTE 150x250 taupe KRONBORG",
  "TM 180x200cm GOLD T30 plain WELLPUR",
  "SHE JEANETTE 240x260 l. grey KRONBORG",
  "MA 140x200cm PLUS F30 DREAMZONE",
  "DCS SCOTTI Flannel SGL grey/white",
  "DCS SONIC SGL blue DBF",
  "CT 160x200cm GOLD C50 storage Grey-41",
  "DCSS TESSA Sateen DBL latte",
  "MA 140x200cm GOLD F30 plain WELLPUR",
  "MA 90x190cm GOLD F115 DREAMZONE",
  "MA 90x200cm GOLD F85 white/blue WELLPUR",
  "MA 90x200cm GOLD F30 dots WELLPUR",
  "COD JUNGEN 4 drawers wild oak",
  "Pedestal THYGE Ø20xH34cm natural",
  "CUR HIRSHOLM 1x135x300 off-white",
  "WG KARLSTAD 15X20cm light green KR",
  "DCSS ANDREA SGL sand/white DBF",
  "HB 140x122cm H70 BUTTONS Grey-28",
  "MA 80x200cm GOLD F30 plain WELLPUR",
  "HB 160x122cm H70 BUTTONS Grey-28",
  "SHE BOLETTE 220x250 grey",
  "DCS PEPPA PIG 2023 SGL rose DBF",
  "HB 140x115cm H10 PLAIN Grey-28",
  "MA 140x200cm GOLD F120 WELLPUR",
  "Shelving unit TISTRUP 3 shel. white/oak",
  "CT 160x200cm PLUS C40 Black-07 DREAMZONE",
  "HB 160x115cm H10 PLAIN Grey-28",
  "MA 80x200cm GOLD F120 WELLPUR",
  "Decorative box EDVARD W12xL12xH6cm glass",
  "Nest of tables FALSLED Ø50/40 glass",
  "CUR SEKKEN 1x140x300 grey w/stripe",
  "MA 90x190cm GOLD F120 WELLPUR",
  "DCS POKEMON 2023 SGL yellow DBF",
  "Side table KRAGEVIG Ø50xH50 hardwood",
  "Wall lamp LINUS W12xL25xH16cm natural",
  "GC chair seat UDSIGTEN sand",
  "DCSS TESSA Sateen SGL latte",
  "DCS FROZEN 2023 SGL purple DBF",
  "TM 120x200cm GOLD T65 white/lt grey",
  "MA 80x200cm GOLD F85 white/blue WELLPUR",
  "HB 160x122cm H70 BUTTONS Grey-23",
  "CUR UNDEN 1x135x300 stripe white",
  "HB 140x115cm H10 PLAIN Grey-34",
  "CT 180x200cm PLUS C40 Black-07 DREAMZONE",
  "DCS MINECRAFT 2023 SGL blue DBF",
  "MA 90x200cm GOLD S100 DREAMZONE",
  "Office chair NIMTOFTE black",
  "Sofa bed chaise longue VEJLBY light sand",
  "Dining chair PEBRINGE velvet grey/black",
  "Dining chair PEBRINGE grey/black",
  "HB 140x115cm H10 PLAIN Black-07",
  "HB 160x115cm H10 PLAIN Grey-34",
  "HB 180x122cm H70 BUTTONS Grey-28",
  "DCS WILMA SGL pink DBF KRONBORG",
  "TM 120x200cm GOLD T30 dots WELLPUR",
  "HB 90x122cm H70 BUTTONS Grey-28",
  "TM 140x200cm GOLD T30 dots WELLPUR",
  "MA 180x200cm GOLD F85 white/dark grey",
  "TM 90x190cm GOLD T30 WELLPUR",
  "FC KARLSTAD 28x30cm light grey KR",
  "CCU DVERGJAMNE 34x36x2 grey",
  "MA 120x200cm GOLD F30 white WELLPUR",
  "Slatted base 90x200cm grey",
  "Sofa bed HOLSTEBRO black",
  "Onion jar HUBERT Ø18xH20cm sand",
  "HB 180x115cm H10 PLAIN Grey-28",
  "TM 120x200cm GOLD T65 white/blue WELLPUR",
  "MA 160x200cm GOLD F35 DREAMZONE",
  "MA 160x200cm GOLD F100 DREAMZONE",
  "MA 180x200cm GOLD S50 DREAMZONE"
) 

  # b, define the 'product_group_level_1' vector  ----------
product_group_level_1 <- c(
  "Furniture",
  "Mattresses",
  "Furniture",
  "Furniture",
  "Furniture",
  "Bathroom",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Furniture",
  "Bed Linen",
  "Bathroom",
  "Mattresses",
  "Furniture",
  "Bathroom",
  "Mattresses",
  "Furniture",
  "Bathroom",
  "Bed Linen",
  "Mattresses",
  "Bathroom",
  "Furniture",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Textiles",
  "Bed Linen",
  "Mattresses",
  "Bathroom",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Textiles",
  "Furniture",
  "Furniture",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Bed Linen",
  "Furniture",
  "Mattresses",
  "Furniture",
  "Mattresses",
  "Bathroom",
  "Textiles",
  "Bed Linen",
  "Bed Linen",
  "Duvets&Pillows",
  "Mattresses",
  "Homeware",
  "Furniture",
  "Furniture",
  "Mattresses",
  "Bathroom",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Bed Linen",
  "Bed Linen",
  "Mattresses",
  "Bed Linen",
  "Bathroom",
  "Mattresses",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Bathroom",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Bed Linen",
  "Homeware",
  "Bed Linen",
  "Furniture",
  "Furniture",
  "Bathroom",
  "Bed Linen",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Bed Linen",
  "Mattresses",
  "Furniture",
  "Bed Linen",
  "Furniture",
  "Bed Linen",
  "Duvets&Pillows",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Bed Linen",
  "Mattresses",
  "Garden",
  "Textiles",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Furniture",
  "Bed Linen",
  "Bed Linen",
  "Bed Linen",
  "Bed Linen",
  "Bed Linen",
  "Mattresses",
  "Bed Linen",
  "Mattresses",
  "Bed Linen",
  "Bed Linen",
  "Mattresses",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Furniture",
  "Textiles",
  "Bathroom",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Furniture",
  "Textiles",
  "Mattresses",
  "Bed Linen",
  "Furniture",
  "Homeware",
  "Garden",
  "Bed Linen",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Textiles",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Mattresses",
  "Furniture",
  "Furniture",
  "Furniture",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Bathroom",
  "Textiles",
  "Mattresses",
  "Mattresses",
  "Furniture",
  "Furniture",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses"
)

# c, define the 'product_category_level_2' vector --------
product_category_level_2 <- c(
  "mirror",
  "Mattresses",
  "sofa bed",
  "bed frame",
  "bed frame",
  "Towel",
  "Mattresses",
  "top mattress",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "dining chair",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "pouffe",
  "Bed Linen",
  "Towel",
  "Mattresses",
  "desk",
  "Towel",
  "Mattresses",
  "bed frame",
  "Towel",
  "Sheets",
  "Mattresses",
  "Towel",
  "footstool",
  "bed frame",
  "Mattresses",
  "Mattresses",
  "shelving unit",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Curtain",
  "Sheets",
  "Mattresses",
  "Towel",
  "Sheets",
  "Mattresses",
  "top mattress",
  "bed frame",
  "TableTextile",
  "bed frame",
  "wardrobe",
  "bed frame",
  "Mattresses",
  "top mattress",
  "Mattresses",
  "Mattresses",
  "top mattress",
  "Mattresses",
  "Sheets",
  "Sheets",
  "sofa bed",
  "Mattresses",
  "stool",
  "Mattresses",
  "Towel",
  "TableTextile",
  "Sheets",
  "Sheets",
  "Pillow",
  "Mattresses",
  "Decoration",
  "pouffe",
  "shelving unit",
  "Mattresses",
  "Towel",
  "Sheets",
  "Mattresses",
  "top mattress",
  "Sheets",
  "Sheets",
  "Bed Linen",
  "Mattresses",
  "Sheets",
  "Towel",
  "Accessorries",
  "nest of tables",
  "continental bed",
  "Mattresses",
  "Mattresses",
  "chest of drawers",
  "Towel",
  "continental bed",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Bed Linen",
  "Bed Linen",
  "Decoration",
  "Sheets",
  "sofa bed",
  "desk",
  "Towel",
  "Sheets",
  "Sheets",
  "top mattress",
  "Mattresses",
  "bed frame",
  "pouffe",
  "Mattresses",
  "Mattresses",
  "bench",
  "Bed Linen",
  "Mattresses",
  "bed frame",
  "Sheets",
  "pouffe",
  "Sheets",
  "Pillow",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "sofa bed",
  "Sheets",
  "Mattresses",
  "Garden table",
  "Curtain",
  "armchair",
  "Mattresses",
  "top mattress",
  "Sheets",
  "Decoration",
  "Sheets",
  "Sheets",
  "Bed Linen",
  "Bed Linen",
  "Sheets",
  "top mattress",
  "Sheets",
  "Mattresses",
  "Bed Linen",
  "Bed Linen",
  "continental bed",
  "Bed Linen",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "Mattresses",
  "chest of drawers",
  "pedestal",
  "Curtain",
  "Towel",
  "Bed Linen",
  "Accessorries",
  "Mattresses",
  "Accessorries",
  "Sheets",
  "Bed Linen",
  "Accessorries",
  "Mattresses",
  "shelving unit",
  "continental bed",
  "Accessorries",
  "Mattresses",
  "decoration",
  "nest of tables",
  "Curtain",
  "Mattresses",
  "Bed Linen",
  "bedside table",
  "lamp",
  "Garden cushion",
  "Bed Linen",
  "Bed Linen",
  "top mattress",
  "Mattresses",
  "Accessorries",
  "Curtain",
  "Accessorries",
  "continental bed",
  "Bed Linen",
  "Mattresses",
  "office chair",
  "sofa bed with chaise longue",
  "dining chair",
  "dining chair",
  "Accessorries",
  "Accessorries",
  "Accessorries",
  "Bed Linen",
  "top mattress",
  "Accessorries",
  "top mattress",
  "Mattresses",
  "top mattress",
  "Towel",
  "TableTextile",
  "Mattresses",
  "Accessorries",
  "sofa bed",
  "Decoration",
  "Accessorries",
  "top mattress",
  "Mattresses",
  "Mattresses",
  "Mattresses"
)

  # d, combine all three vectors into a data frame named 'lookup'
lookup <- data.frame(
  product_title = product_title,
  product_group_level_1 = product_group_level_1,
  product_category_level_2 = product_category_level_2,
  stringsAsFactors = FALSE
)

  # e, Print to confirm
print(lookup, n = nrow(lookup))

  # f, now the actual imputation part
  # create data_joined by joining data with lookup on product_title
data <- data %>%
  left_join(lookup, by = "product_title", suffix = c("", ".lkp")) %>%
  # replace NAs in original columns with the lookup columns
  mutate(
    product_group_level_1 = coalesce(product_group_level_1, product_group_level_1.lkp),
    product_category_level_2 = coalesce(product_category_level_2, product_category_level_2.lkp  )
  ) %>%
  # remove the extra lookup columns
  dplyr::select(-ends_with(".lkp"))

# Convert the 'date' column to Date type
data$date <- as.Date(data$date, format = "%d.%m.%Y")
# Convert all non-numeric columns (besides 'date') to factors
data[] <- lapply(data, function(x) if(is.character(x)) as.factor(x) else x)
# Impute NA customer id's with 999999999 for now. Will be removed in CLV probably
data <- data %>%
  mutate(customer_id = replace_na(customer_id, 9999999999))

sum(is.na(data))

skimr::skim(data)
```

## RFM

```{r #APPLICATION1}

#Visualization of the 3 rfm variables

data_clean_rfm <- data[data$customer_id != 9999999999, ]

# Revenue histogram
hist(data_clean_rfm$revenue, main = "Histogram of Revenue", col = "blue", border = "black")
# Check for NAs
sum(is.na(data_clean_rfm$revenue))      # Count NAs in revenue



# Customer ID bar plot - Delete later, useless, including the NA checks
cust_freq <- table(data_clean_rfm$customer_id)

# Filter to only those with frequency >= 10
cust_freq_filtered <- cust_freq[cust_freq >= 35]

barplot(cust_freq_filtered, main = "Customer ID Distribution", col = "blue", las = 2, ylim = c(0, 80))


# Date bar plot
barplot(table(data_clean_rfm$date), main = "Date Distribution", col = "blue")
sum(is.na(data_clean_rfm$date))         # Count NAs in date

#CAN BE SEEN no NAs and also no negative variables, can proceed with the RFM model running
 
```

```{r #Application 1 - RFM model}
# 1. Aggregate to Order-Level Data
order_level_data <- data_clean_rfm %>%
  dplyr::select(date, order_id, customer_id, revenue) %>%  # keep only relevant columns
  dplyr::group_by(order_id, customer_id, date) %>%         # group by order_id, customer_id, and date
  dplyr::summarise(total_revenue = sum(revenue, na.rm = TRUE), .groups = "drop")  # sum revenue and ungroup

# 2. Set Analysis Date (using the day after your last order date)
analysis_date <- as.Date("2025-01-01")  

# 3. Run the RFM Analysis using the Aggregated Data
# Note: We now use 'total_revenue' instead of 'revenue'
rfm_result <- rfm_table_order(order_level_data, 
                              customer_id, 
                              date, 
                              total_revenue, 
                              analysis_date = analysis_date)

# 4. Visualize RFM Results
rfm_plot_heatmap(rfm_result)
rfm_plot_bar_chart(rfm_result)

# 5. Define RFM Segment Categories and Thresholds
segment_names <- c("Champions", "Loyal Customers", "Potential Loyalist", 
                   "New Customers", "Promising", "Need Attention", "About To Sleep", 
                   "At Risk", "Can't Lose Them", "Lost")

# For Recency
summary(rfm_result$rfm$recency_score)
hist(rfm_result$rfm$recency_score, main = "Recency Distribution", xlab = "Recency")

# For Frequency
summary(rfm_result$rfm$frequency_score)
hist(rfm_result$rfm$frequency_score, main = "Frequency Distribution", xlab = "Frequency")

# For Monetary
summary(rfm_result$rfm$monetary_score)
hist(rfm_result$rfm$monetary_score, main = "Monetary Distribution", xlab = "Monetary")

quantile(rfm_result$rfm$recency_score, probs = seq(0, 1, 0.2))
quantile(rfm_result$rfm$frequency_score, probs = seq(0, 1, 0.2))
quantile(rfm_result$rfm$monetary_score, probs = seq(0, 1, 0.2))

# For each customer, count the number of orders (each row in order_level_data is an order)
customer_orders <- order_level_data %>%
  dplyr::group_by(customer_id) %>%
  dplyr::summarise(num_orders = dplyr::n(), .groups = "drop")

# Count how many customers fall into each order count bucket
freq_dist <- customer_orders %>%
  dplyr::count(num_orders)

print(freq_dist)


recency_lower   <- c(4, 2, 3, 4, 3, 2, 2, 1, 1, 1)
recency_upper   <- c(5, 5, 5, 5, 4, 3, 3, 2, 1, 2)
frequency_lower <- c(5, 5, 3, 1, 1, 1, 1, 1, 1, 1)
frequency_upper <- c(5, 5, 5, 1, 1, 1, 1, 1, 1, 1)
monetary_lower  <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
monetary_upper  <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)

# 6. Apply Segmentation Based on RFM Scores
segment <- rfm_segment(rfm_result, 
                       segment_names,
                       recency_lower, recency_upper,
                       frequency_lower, frequency_upper,
                       monetary_lower, monetary_upper)

# 7. View the Segmented Customers
head(segment)

# 8. Visualize RFM Segments
rfm_plot_median_recency(segment, sort = TRUE)
rfm_plot_median_monetary(segment, sort = TRUE)
rfm_plot_median_frequency(segment, sort = TRUE)

```

---
# -------------------------------
# Image 1: RFM Heatmap Visualization
# -------------------------------
# Observation:
# - The heatmap shows a concentration of customers in the lower recency and monetary score areas.
# - A few cells (likely corresponding to Champions) stand out with high recency and monetary scores.
#
# Recommendation for JYSK Romania:
# - Focus on retaining the small, high-value segment (Champions) with exclusive offers and premium customer service.
# - Develop targeted re-engagement campaigns for customers in low-score zones to improve their purchase recency and spending.
# -------------------------------


# -------------------------------
# Image 2: RFM Bar Chart of Scores
# -------------------------------
# Observation:
# - The bar chart shows that the frequency score is highly skewed: almost all customers have a frequency score of 1,
#   while a very small portion jumps to 5.
#
# Recommendation for JYSK Romania:
# - Since most customers are one-time buyers, create initiatives (e.g., loyalty programs, follow-up promotions) to encourage repeat purchases.
# - For the rare repeat buyers (frequency = 5), consider upselling and cross-selling strategies to maximize their lifetime value.
# -------------------------------


# -------------------------------
# Image 3: Median Recency by Segment
# -------------------------------
# Observation:
# - The median recency values across segments reveal that Champions and Loyal Customers have high recency scores (near 5),
#   while segments like Lost or At Risk show very low recency.
#
# Recommendation for JYSK Romania:
# - Maintain strong engagement with customers in high recency segments (Champions and Loyal Customers) by offering regular incentives.
# - Launch targeted reactivation campaigns for segments with low recency to try to bring them back into active status.
# -------------------------------


# -------------------------------
# Image 4: Median Monetary by Segment
# -------------------------------
# Observation:
# - The monetary median is highest for segments such as Champions and “Can't Lose Them,” indicating these groups contribute most revenue.
# - Lower monetary values are observed in segments like New Customers or Lost.
#
# Recommendation for JYSK Romania:
# - Allocate more resources to premium and cross-selling opportunities for high-spending segments.
# - Introduce personalized discounts or bundle offers for segments with lower monetary scores to increase their average order value.
# -------------------------------


# -------------------------------
# Image 5: Median Frequency by Segment
# -------------------------------
# Observation:
# - The median frequency data is extremely polarized, with Champions and Loyal Customers showing significantly higher frequencies,
#   and almost all other segments having a frequency of 1.
#
# Recommendation for JYSK Romania:
# - Design loyalty programs aimed at converting one-time buyers into repeat customers.
# - For segments with low frequency, consider implementing reminder emails, special repeat-purchase discounts, or loyalty points.
# -------------------------------


# -------------------------------
# Image 6: Distribution of Segments (e.g., a table/chart showing customer counts per segment)
# -------------------------------
# Observation:
# - The distribution might reveal that a large portion of customers fall into the New or Lost segments,
#   with only a small percentage in the high-value segments.
#
# Recommendation for JYSK Romania:
# - Invest in re-engagement strategies for the New and Lost segments—such as targeted promotions or win-back campaigns.
# - Use segmentation insights to tailor marketing messages for each group, with a focus on moving customers from low-value to high-value segments.
---

```{r some data exploration}
#APPLICATION 2

#starting with descriptive statistics

#Cleaning the data on na customers but also revenue above 0
data_application2 <- data %>% 
  filter(!is.na(customer_id))

#DESCRIPTIVE STATISTICS

#### NR of orders per month ####
# Extract month and year from the date
data_application2$date_month <- format(as.Date(data_application2$date), "%Y-%m")

# Plot number of orders per month
ggplot(data_application2) + 
  geom_bar(aes(x = date_month), colour = "grey20", fill = "grey80") + 
  ggtitle("Orders per Month") + 
  xlab("Month") +
  ylab("Number of Orders") +
  theme(panel.background = element_rect(fill = "grey94")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text = element_text(size = 14, angle = 45, hjust = 1),  # Rotate text to 45 degrees
        axis.title = element_text(size = 14))


#### NR of orders per customer####

# Truncate orders per customer
# Step 1: Calculate the number of orders per customer
order_count_per_customer <- table(data_application2$customer_id)

# Step 2: Truncate the number of orders to 15
order_count_truncate <- pmin(order_count_per_customer, 15)

# Step 3: Create a data frame for plotting
order_summary <- as.data.frame(table(order_count_truncate))

# Step 4: Plot the number of orders per customer using a bar plot
ggplot(order_summary, aes(x = as.factor(order_count_truncate), y = Freq)) + 
  geom_bar(stat = "identity", fill = "grey80", colour = "grey20") + 
  ggtitle("Orders per Customer (Truncated to 15)") +  
  xlab("Total Number of Orders") +
  ylab("Number of Customers") +
  theme(panel.background = element_rect(fill = "grey94")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text = element_text(size = 14), axis.title = element_text(size = 14))

####  Days Between Orders  ####

# Sort data by customer_id and date
data_application2 <- data_application2 %>%
  arrange(customer_id, date)

# Calculate days since the last order for each customer
data_application2 <- data_application2 %>%
  group_by(customer_id) %>%
  mutate(days_since_last_order = as.numeric(difftime(date, lag(date), units = "days"))) %>%
  ungroup()

# Replace NA values (for the first order) with 0
data_application2$days_since_last_order[is.na(data_application2$days_since_last_order)] <- 0

# Remove customers with only one order (days_since_last_order = 0)
data_application2_filtered <- data_application2 %>%
  filter(days_since_last_order > 0)

# Now create the subset for the relevant columns (order_id, customer_id, days_since_last_order)
daysdata <- subset(data_application2_filtered[, c("order_id", "customer_id", "days_since_last_order")])

# Calculate the mean of the days_since_last_order for each customer
daysdatamean <- aggregate(days_since_last_order ~ customer_id, data = daysdata, FUN = mean)

# Plot histogram of the days between orders
ggplot(daysdatamean) + 
  geom_histogram(aes(x = days_since_last_order), binwidth = 5, colour = "grey20", fill = "grey80") +
  ggtitle("Days Between Orders customers with at least 2 orders") +  
  xlab("Average Days Between Orders") +
  ylab("Number of Customers") +
  theme(panel.background = element_rect(fill = "grey94")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text = element_text(size = 17, face = "plain"),
        axis.title = element_text(size = 17, face = "plain"),
        title = element_text(size = 16, face = "plain"))


#### Postal code distribution - also matching with acutal cities #####


# Read the postal codes data file
postal_codes <- read.csv("postal_codes_Romania.csv")

# Convert all columns to factors in the postal_codes dataframe
postal_codes[] <- lapply(postal_codes, as.factor)

# Convert all non-numeric columns to factors in the data_application2 dataframe
data_application2[] <- lapply(data_application2, function(x) if (is.character(x)) as.factor(x) else x)

# Ensure that the postal_code and customer_zip_code are both of the same type (character for this case)
postal_codes$postal_code <- as.character(postal_codes$postal_code)
data_application2$customer_zip_code <- as.character(data_application2$customer_zip_code)

# Filter out rows with NA in both customer_zip_code and postal_code columns
data_application2_filtered <- data_application2 %>%
  filter(!is.na(customer_zip_code))  # Remove rows with NA in customer_zip_code
app2_subset <- subset(data_application2_filtered[, c("customer_id", "customer_zip_code")])
app2_subset <- app2_subset%>%
  dplyr::rename(postal_code = customer_zip_code)

str(app2_subset)

#filter the postal one
postal_codes_filtered <- postal_codes %>%
  filter(!is.na(postal_code))  # Remove rows with NA in postal_code
postal_subset <- subset(postal_codes_filtered[, c("postal_code", "Judet")])

str(postal_subset)

# Assuming app2_subset and postal_subset are already defined
# Join the app2_subset with postal_subset based on the postal_code
joined_data <- app2_subset %>%
  dplyr::left_join(postal_subset, by = "postal_code") %>%
  mutate(matched_postal_code = coalesce(postal_code, postal_code))


str(joined_data)

#### PLOT ####

# Replace NA in 'Judet' with 'Bucuresti'
joined_data$Judet[is.na(joined_data$Judet)] <- "Bucuresti"

# Check the result to make sure NAs were replaced
sum(is.na(joined_data$Judet))  # Should return 0 if all NAs are replaced



# Ensure 'Judet' is treated as a character and 'customer_id' is numeric
joined_data$Judet <- as.character(joined_data$Judet)

# Ensure the encoding of Judet is correct (UTF-8)
joined_data$Judet <- iconv(joined_data$Judet, from = "UTF-8", to = "ASCII//TRANSLIT")

# Count the occurrences of each Judet
Judet_counts <- table(joined_data$Judet)

# Sort the counts in descending order
Judet_counts_sorted <- sort(Judet_counts, decreasing = TRUE)

# Create a bar plot of the sorted Judet counts
barplot_heights <- barplot(Judet_counts_sorted, 
        main = "Count of Customers per Judet (Sorted)", 
        xlab = "Judet", 
        ylab = "Count of Customers", 
        las = 2,  # Rotate x-axis labels for readability
        col = "steelblue",
        cex.names = 0.6)  # Reduce the size of X-axis labels

# Count how many NAs are in the 'Judet' column
na_count_judet <- sum(is.na(joined_data$Judet))

# Print the result
print(na_count_judet)


#### BONUS #####

# Install and load the necessary libraries
# install.packages("tidygeocoder")
# install.packages("sf")
# install.packages("ggplot2")
library(tidygeocoder)
library(sf)
library(ggplot2)

# Get unique Judet names (remove duplicates) and convert to dataframe
unique_judets_df <- data.frame(Judet = unique(joined_data$Judet))

# Geocode Judet names to get latitude and longitude
# Specify 'address' column explicitly in geocode function
geocoded_judets <- tidygeocoder::geocode(unique_judets_df, address = "Judet", method = 'osm')

# Merge geocoded data back with customer data to include coordinates
judet_geo_data <- data.frame(Judet = unique_judets_df$Judet, 
                             Latitude = geocoded_judets$lat, 
                             Longitude = geocoded_judets$long)

# Update Latitude and Longitude for the row "Alba"
judet_geo_data$Latitude[judet_geo_data$Judet == "Alba"] <- 46.1559
judet_geo_data$Longitude[judet_geo_data$Judet == "Alba"] <- 23.5556

# Count occurrences of each Judet in your original dataset
Judet_counts <- table(joined_data$Judet)
Judet_counts_df <- data.frame(Judet = names(Judet_counts), Count = as.vector(Judet_counts))

# Merge the counts with the geocoded data
geo_count_data <- merge(judet_geo_data, Judet_counts_df, by = "Judet")  # Convert to sf object for spatial plotting

# Convert the data to an sf object for spatial plotting
geo_count_sf <- st_as_sf(geo_count_data, coords = c("Longitude", "Latitude"), crs = 4326)

# Plot the map with customer counts
ggplot(geo_count_sf) +
  geom_sf(aes(color = Count, size = Count), alpha = 0.7) +
  scale_color_gradientn(
    colors = c("navy", "navy", "red"),
    limits = range(geo_count_sf$Count)
  ) +
  # Increase circle sizes by setting a larger size range
  scale_size_continuous(range = c(3, 10)) +
  labs(title = "Customer Counts per Judet", fill = "Customer Count") +
  theme_minimal() +
  # Remove grid lines from the plot
  theme(
    legend.position = "bottom",
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank()
  )


View(unique_judets_df)

```

###CLV Model

```{r #Application 2 - CLV - setting up the dataset}
      
      #data3 - clv_subset
      #data1 - data_btyd
      #data2 - data_btyd_recency

library(dplyr)
library(lubridate)

#getting the clv data neccesary
clv_data <- data %>%
  group_by(customer_id) %>%
  dplyr::summarise(
    sales = revenue,  
    date = date  
  ) %>%
  ungroup()

# Getting the helper collumn: first time purchase
clv_data <- clv_data %>%
  arrange(customer_id, date) %>%  # Ensure data is sorted by date
  group_by(customer_id) %>%
  mutate(first_purchase_date = min(date)) %>%  # Get the first purchase date per customer
  ungroup()

#Removing the helper collumns and filtering so no newcomers in the test set
clv_data <- clv_data %>%
  dplyr::filter(first_purchase_date < as.Date("2024-09-01")) %>%
  dplyr::select(-first_purchase_date)  

clv_data <- as.data.frame(clv_data)

# Remove customer with ID 9999999999
clv_data <- clv_data %>%
  filter(customer_id != 9999999999)


 # Select subset
    clv_subset <- mutate(clv_data[,c(1,2,3)], date =as.Date(date)) 
    names(clv_subset) <- c("cust", "sales", "date") 
    length(unique(clv_subset$cust)) 
    data_btyd <- dc.MergeTransactionsOnSameDate(clv_subset)

    
    #### BTYD prepping data final
    
          end.of.cal.period <- as.Date("2024-09-01")
          data_btyd_recency <- dc.ElogToCbsCbt(data_btyd, per="day", T.cal = end.of.cal.period,  statistic = "freq") 
          cal.cbs <- as.matrix(data_btyd_recency[[1]][[1]])
          variable.names(cal.cbs)
        
```

```{r BG/NBD using calibration matrix}
             #### BG/NBD
              ## Model estimation ##
              # Estimate parameters for the BG/NBD model using the calibration matrix
              params.bg <- bgnbd.EstimateParameters(cal.cbs, max.param.value = 10000)
              params.bg  # Print the estimated parameters
              
              # Log likelihood of the model
              LL.bg <- bgnbd.cbs.LL(params.bg, cal.cbs)
              LL.bg
              
              ## Plots ##
              # Calibration period fit - aggregated plots
              bgnbd.PlotFrequencyInCalibration(params.bg, cal.cbs, 7)       

```

Plot shows:

0 repeat purchases - most customers only purchased once The model is able to track actual sales well, but is slightly worse at tracking 1 and 2 repeat purchases

This likely typical in a low-frequency sales environment such as furniture - it can be argued that most people make only one large purchase and then disappear for a long time

```{r Gamma-Gamma monetary model}

              
                   
  
    #### Gamma-gamma monetary model
          # Create data
          cal.cbs1 <- as.data.frame(cal.cbs) # Convert calibration matrix to a data frame
          cal.cbs1 <- tibble::rownames_to_column(cal.cbs1, "cust") # Add row names as a customer ID column
          cal.cbs1$cust <- as.integer(cal.cbs1$cust) # Convert customer IDs to integer type
          data_gamma_train<- subset(clv_subset, date < as.Date("2024-09-01")) # Subset the original data to include only transactions before a specified date
          df_gamma_train <- data_gamma_train %>% group_by(cust) %>% summarise_at(vars(sales), list(m.x = mean)) # Group data by customer and calculate the mean sales per 
          cal.cbs.gamma <- merge(cal.cbs1, df_gamma_train, by = "cust") # Merge calibration data with transaction means
          # hence, m.x is the average spending per transaction for each customer
          
          # Obtain parameters
          ave.spend <- cal.cbs.gamma$m.x # Extract the average spending per transaction for each customer
          tot.trans <- cal.cbs.gamma$x # Extract the total number of transactions for each customer
          ave.spend <- ave.spend[which(tot.trans >0)] # Filter for customers with more than 0 transactions
          tot.trans <- tot.trans[which(tot.trans >0)] # Filter for customers with more than 0 transactions
          params_gamma <- spend.EstimateParameters(m.x.vector = ave.spend, x.vector = tot.trans) # Estimate the Gamma-Gamma model parameters
          params_gamma
          
          # Plot
          spend.plot.average.transaction.value(
            params = params_gamma,
            m.x.vector = ave.spend,
            x.vector = tot.trans,
            xlab = "Average Transaction Value",
            ylab = "Marginal Distribution of Average Transaction Value",
            title = "Actual vs. Expected Average Transaction Value Across Customers"
          )
```

```{r assessing independance for CLV}
        library(ggpubr)
          
              ave.spend #average transaction value in training data
              tot.trans.trunc <- tot.trans
              tot.trans.trunc[tot.trans.trunc > 7] = 7   # Cap the transactions at 7 to limit extreme values
              data.ass <- data.frame(tot.trans.trunc, ave.spend) # Create a new data frame for assessing independence
              
              # Plot a boxplot to visually evaluate the independence of average spend and number of transactions
              ggboxplot(data.ass, x = "tot.trans.trunc", y = "ave.spend", 
                        fill = "grey 88", ylab = "Average purchase value", xlab = "Number of repeat purchases") +
                theme(axis.text=element_text(size=14, face = "plain"),
                      axis.title=element_text(size=15, face = "plain"))
              

              # Calculate the correlation between average spend and number of transactions to evaluate the independence assumption
              cor(data.ass$ave.spend, data.ass$tot.trans.trunc)
              
```

Plot comments: Customers with only 1 repeat purchase tend to have wider spread in purchase value, many high outliers

As number of repeat purchases increase, the variance in avg. purchase value decreases which could be argued to be expected - we see no movement in the median values of the repeat purchases. As no. repeat purchases increase, variance decreases which could suggest a more stable average.

Correlation comments: Correlation is very close to 0, which should indicate no meaningful correlation and supports the independence assumption of gamma-gamma

```{r setting up the data for next year prediction}
    # BG/NBD 
    T.star <- max(data_btyd_recency$holdout$cbs[,"T.star"])
      cal.cbs1$yearpred.bg <- round(bgnbd.ConditionalExpectedTransactions(
      params.bg,
      T.star = T.star,
      x = cal.cbs1$x,
      t.x = cal.cbs1$t.x,
      T.cal = cal.cbs1$T.cal,
      hardie = TRUE),2)
      
    cal.cbs.gamma$spend <- spend.expected.value(
      params = params_gamma, 
      m.x = cal.cbs.gamma$m.x,
      x =cal.cbs.gamma$x) 
    
    
  
      data_gamma_test <- subset(data_btyd, date >= as.Date("2024-09-01"))
      df_gamma_test <- data_gamma_test %>% 
                        group_by(cust) %>% 
                        summarise_at(vars(sales), 
                        list(m.x.avg.real.sale.test = mean)) 
      cal.cbs.gamma.test <- merge(cal.cbs.gamma, df_gamma_test, by = "cust", all.x = TRUE)

      Freq <- as.data.frame(table(data_gamma_test$cust))
      cal.cbs.gamma.test <- merge(cal.cbs.gamma.test, Freq, by.x = "cust", by.y = "Var1", all.x = TRUE)
  
      cal.cbs.gamma.test[is.na(cal.cbs.gamma.test)] = 0
      
  
      cal.cbs.gamma.test <- merge(cal.cbs.gamma.test, cal.cbs1, by.x = "cust", by.y = "cust", all.x = TRUE)
      cal.cbs.gamma.test
 
         
     # calculate expected CLV (1.year predictions)
      cal.cbs.gamma.test$CLV.pred.bg <- cal.cbs.gamma.test$spend * cal.cbs.gamma.test$yearpred.bg # BG/NBD
      
          
    # calculate real CLV based on actual sales and transaction frequency
      cal.cbs.gamma.test$CLV.real <- cal.cbs.gamma.test$m.x.avg.real.sale.test * cal.cbs.gamma.test$Freq

```

```{r Expected CLV and real CLV - some error metrics}
  # Performance measures: expected CLV vs. real CLV
       rmse_bg <- rmse(cal.cbs.gamma.test$CLV.real, cal.cbs.gamma.test$CLV.pred.bg)
       mae_bg <- mae(cal.cbs.gamma.test$CLV.real, cal.cbs.gamma.test$CLV.pred.bg)
  
 
      #### Summary of evaluation (MSE and RMSE)
      models_rmse = cbind(rmse_bg)
      models_mae = cbind(mae_bg)
      models_stats = rbind(models_rmse, models_mae)
      rownames(models_stats) = c("RMSE", "MAE")
      colnames(models_stats) = c("BG/NBD")
      round(models_stats,3)
```

```{r histogram for CLV real}
      # Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real, 
     main = "Histogram of CLV.real", 
     xlab = "CLV Value", 
     col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
     border = "black", 
     breaks = 50, 
     xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
     ylim = c(0, 90000))  # Adjust y-axis range for better visualization

# Add a legend
legend("topright", 
       legend = c("CLV.real"), 
       fill = c(rgb(0, 0, 1, 0.5)))

# add extra plot - density plot instead of histogram
plot(density(cal.cbs.gamma.test$CLV.real),
     main = "Density Plot of CLV.real",
     xlab = "CLV Value",
     col = "blue",
     lwd = 2)

# density plot that should somewhat handle left skew
plot(density(cal.cbs.gamma.test$CLV.real[cal.cbs.gamma.test$CLV.real < 2000]),
     main = "Density Plot of CLV.real (Capped at 2000 RON)",
     xlab = "CLV Value",
     col = "blue",
     lwd = 2)


```

```{r plot for comparing predicted bg model vs real clv}

ggplot(cal.cbs.gamma.test, aes(x = CLV.pred.bg, y = CLV.real)) +
  geom_point(aes(color = "Data Points"), alpha = 0.5) +  
  geom_smooth(aes(color = "Trend Line"), method = "lm", se = FALSE) +  
  labs(title = "Predicted vs Real CLV",
       x = "Predicted CLV (CLV.pred.bg)",
       y = "Real CLV (CLV.real)",
       color = "Legend") +  
  scale_color_manual(values = c("Data Points" = "blue", "Trend Line" = "red")) +
  theme_minimal()


```

Scatterplot compares each customer's predicted clv using BG/NBD and gamma-gamma model with their actual clv observed during the 1 year holdout period

positive trend - indicates model is able to capture the overall relationship between predicted and actual value - customers with higher pred.clv tend to spend more - this is what we should expect as customers with a high real clv are spending more at the retailer such as jysk

dispersion - for customers with lower pred.clv. Some of these are high value because their real clv is high (nearing top left corner of the plot) This could mean that the model underestimates a small group of high-spending customers or these customers may have unpredictable spending patterns

## Association Rules Mining

-   The association rules will delve into the products inter-relationships and provide insights about the products bought together

```{r Association - cleaning the data}
# Take dataset
library(skimr)
data_asm <- data

#Cleaning the data on na customers but also revenue above 0
data_asm <- data_asm %>% 
  filter(!is.na(customer_id) & revenue > 0)

skim(data_asm)

```

```{r Association - preparing the transaction dataframe}
transacData <- plyr::ddply(data_asm, c("order_id","date"),
                           function(df1){
                             # wrap each product title in double quotes
                             titles <- paste0('"', df1$product_title, '"')
                             paste(titles,collapse = ";")
                             
                             }) 
#changed delimiter from "," to ";" as some product titles contains ","


head(transacData)

transacData$order_id <- NULL

transacData$date <- NULL

colnames(transacData) <- c("items")
head(transacData)

basket_list <- strsplit(transacData$items, ";")

# remove leading/trainling whitespace and surroundind quotes
basket_list <- lapply(basket_list, function(x) trimws(gsub('^"|"$', '', x)))

library(arules)
tr <- as(basket_list, "transactions")

summary(tr)

```

```{r PLot for the top 20 relative and absolute items frequency}
library(RColorBrewer)

# top 10 most frequently bought items
itemFrequencyPlot(tr,topN=10,type="absolute",col=brewer.pal(8,'Pastel2'), 
                  main="Absolute Item Frequency Plot")

# top 25 most frequently bought items
itemFrequencyPlot(tr,topN=25,type="relative",col=brewer.pal(8,'Pastel2'),
                  main="Relative Item Frequency Plot")

```

-   most frequent products bought by the customers : DUV 1320g BAGN warm 200x220cm : 1662 Folding table KULESKOG W75xL180 white : 1255 Clothes rail GUDME double black/chrome : 1243\
    PIL 750g TRONFJELLET 50x70cm: 1188 BTH ENGBLOMME 220x240 rose" 1050

```{r confidence 0.8  and support 0.001}
# We set support >= 0.1% of all transactions - A rule should occur in approx. 163 orders
# we set confidence >= 80%
# A rule can at most contain 10 items
# This is likely a very strict rule and we expect very few itemsets to pass this filter

association.rules <- apriori(tr, 
                             parameter = list(supp=0.001, conf=0.8,maxlen=10))

summary(association.rules)

```

```{r inspecting the rules at 0.8 confidence}
inspect(association.rules)

sortedRules1 <- sort(association.rules,by="lift",decreasing=TRUE)

inspect(sortedRules1) 
```

- We see that 1 rule passed the filters of confidence = 0,8, support = 0,001
- This means that most of items aren't purchased enough together to exceed these thresholds

- When a customer buys "Back cushion DUNHAMMER 35x75 beige" then they are also likely to buy "BTH DUNHAMMER 220x240 beige"

- support
- The support for this rule appears in 0,15% of all transactions (244 orders out of 163,765)

- confidence
- in 80,5% of the cases where someone bought the cushion, they also bought the throw/blanket 

- Coverage
- onl 0,185% of orders contain the cushion

- Lift
- The lift of this rule is = 168,4 and is > 1 which indicates a strong association.
- When a customer purchases the cushion then there is an increased likelihood that they
- will also purchase the blanket and that the pair is not a coincidence

- The product title indicates that they are most likely part of the same product line
- named DUNHAMMER, and customers often purchase such goods together

- Some recommendations
- use this insight in recommendation models/engines on the website, so customers
- are recommened one of these products when they look at the other

- Plan in-store shelf placement: Put products from the same product line near each other
- such as this pair to make the co-purchase easier for customers


```{r 0.6 confidence and support 0.0005}
# we now relax the thresholds, so confidence = 0,6
# support = 0,0005 (approx. 85 orders where a rule must be present)

relaxed.rules <- apriori(tr, 
                             parameter = list(supp=0.0005, conf=0.6,maxlen=10))
summary(relaxed.rules)

inspect(relaxed.rules)

sortedRules2 <- sort(relaxed.rules,by="lift",decreasing=TRUE)

inspect(sortedRules2) 

```

-   Due to the relaxed support and confidence now a list of 13 of items bought together.

- the strongest rule based on a lift above 1000 is when customers purchase a bowl set and a large sized plate set
- then they are also likely to purchase a medium sized plate set. These 3 items all come from the same product line "FERDUS"

- The support for this rule is 0,00052 and occurs in 85 orders. 
- The confidence = 88,5% is high, so when those two items are bought togeter, the third item is
- almost always included

- Patterns
- FERDUS plates/bowls belong to the same product line/series - a dinnerware set
- it thus makes sense to bundle these products or upsell them

- We also notice that being either the medium plate or the large plate in isolation leads
- to customers purchasing the other as well with lifts at 954 and 889 respectively.

- Looking at another product series, we see that when customers purchase the MALUNG towels they
- are also likely to purchase them in different colours, (dark, grey) -> (beige) and opposite

- DUNHAMMER and ENGBLOMME cushions lead to purchase of blankets from same brand as well
- indicates co-purchase within the product series

- Opportunities
- Offer bundling of 3 items that are often bought together such as fx. dinnerware like FERDUS

- Promote/advertise a "look" with product series such as the FERDUS dinnerware, MALUNG towels
- and DUNHAMMER / ENGBLOMME series




```{r plot for rules with confidence above 0.4 }

library(arulesViz)
subRules <- relaxed.rules[quality(relaxed.rules)$confidence > 0.4]
plot(subRules)
```

-   The plot provides the situation of the 13 rules found previously and mapping them on the scale of confidence and support, the best ones are the ones situated at the highest confidence and highest support area to the top right. The least effective rules are in the bottom left corner where confidence and support is at their lowest or near the threshold.

- we see most rules move on the confidence axis but are less able to move on the support axis, which is likely due to
the high amount of orders in the dataset. The lift is indicated by the color intensity of the dots. The one rule identified with the strict thresholds are identified
in the top right corner of high confidence and high support, but with lower lift. 

```{r plot for 13 rules and their respective confidence and support}
# interactive plot

plot(subRules, method = "two-key plot")

```

- The plot provides a representation of the order 2 and 3 rules. 
- Order 2: Rules that involve 2 items: A -> B 
- Order 3: Rules that involve 3 items: A,B -> C


The order 3 ones are clearly low in support and lower in confidence compared the order 2 ones, indicating that order 2 rules
are stronger. We also notice the 1 very strong rule that was identified with the strict threshold, which is scoring above 0,8 in 
confidence and near 0,0015 in support to the far right. 

```{r plotting on the html widget library}

top10subRules <- head(subRules, n = 13, by = "confidence")
plot(top10subRules, method = "graph",  engine = "htmlwidget")
```

- Clusters:
- FERDUS cluster in the center - suggests a product line with interrelated rules
- We see that arrows go in both directions, which could indicate that products are strongly associated
- regardless of the order they are purchased in - this could benefit from promoting a "buy together" campaign

- Separate clusters:
- DUNHAMMER, ENGBLOMME, MALUNG
- These are not connected to each other but have their own small ecosystems
- The relationships do not seem to go in both directions meaning that the purchase of the 
- the first product is important if the other item is also bought
- example: if the cushion is purchased, then the blanket often follows, but not always the other way around


## Recommender system

In the following we implement an IBCF model with some parameter tuning, and basic cross-validation just to emphasize that the tuning of these models can be a really long and resource-heavy experimentation process.

```{r}
# Load packages
library(dplyr)
library(tidyr)
library(recommenderlab)

# a, data preparation: creating binary interaction dataset (purchases)
data_rcs <- data
interactions <- data_rcs %>%
  dplyr::select(customer_id, product_title) %>%
  distinct() %>%
  mutate(purchased = 1)

# b, sampling a subset of customers
set.seed(123)
sample_customers <- sample(unique(interactions$customer_id), 1000)
sample_data <- interactions %>%
  filter(customer_id %in% sample_customers)

# c, filtering to top 100 most purchased products
top_products <- sample_data %>%
  dplyr::count(product_title, sort = TRUE) %>%
  slice_head(n = 100) %>%
  pull(product_title)
sample_data <- sample_data %>%
  filter(product_title %in% top_products)

# d, creating user-product matrix (1 = purchased)
# Transforms the data from long format to a wide matrix where:
# Rows represent customers
# Columns represent products
# Cell values are 1 (purchased) or 0 (not purchased)

interaction_matrix <- sample_data %>%
  tidyr::pivot_wider(
    names_from = product_title,
    values_from = purchased,
    values_fill = list(purchased = 0)
  )

# e, Convert to matrix format
customer_ids <- interaction_matrix$customer_id
interaction_matrix <- interaction_matrix %>% dplyr::select(-customer_id)
mat <- as.matrix(interaction_matrix)
rownames(mat) <- customer_ids

# f, convert to binaryRatingMatrix
binary_ratings <- as(mat, "binaryRatingMatrix")

# Remove sparse customers/products
# filters out customers who bought fewer than 3 products
# filters out products purchased by fewer than 6 customers
filtered_ratings <- binary_ratings[rowCounts(binary_ratings) > 2, colCounts(binary_ratings) > 5]

# define a grid of k parameters to test
k_values <- c(10, 20, 30, 50, 100)

# create a function to evaluate models with specific parameters
# Repeatedly splitting data into 80% training and 20% testing (3 times)
# Building an Item-Based Collaborative Filtering (IBCF) model with the current k value
# Making recommendations for test users
# Returning the recommendations for evaluation

evaluate_IBCF <- function(ratings_matrix, k) {
  # here we use a simple 80/20 split repeated 3 times (simpler than k-fold, due to computation efficiency and also just to demonstrate what can be played around with to improve model performance)
  results <- replicate(3, {
    # create train/test split
    set.seed(sample.int(1000, 1))  # different seed each time for variety here as well
    which_train <- sample(c(TRUE, FALSE), nrow(ratings_matrix), replace = TRUE, prob = c(0.8, 0.2))
    train_data <- ratings_matrix[which_train, ]
    test_data <- ratings_matrix[!which_train, ]
    
    # Build model with current k
    model <- Recommender(train_data, method = "IBCF", parameter = list(k = k))
    
    # Predict top 5 recommendations
    preds <- predict(model, test_data, n = 5)
    
    # Get performance metrics using recommenderlab
    getList(preds)  # Return the list to make manual evaluation easier
  })
  
  return(results)
}

# test each k value and store results
cat("Testing different k values for IBCF model\n")
performance_results <- list()

for (k in k_values) {
  cat("Testing k =", k, "\n")
  tryCatch({
    results <- evaluate_IBCF(filtered_ratings, k)
    performance_results[[as.character(k)]] <- results
    cat("  Completed successfully\n")
  }, error = function(e) {
    cat("  Error:", e$message, "\n")
  })
}

# just set best k to 30 to fall back to if the function fails, and also to see whether it works. If best k changes from 30 which it does, to 10, then our custom function works.
best_k <- 30

if (length(performance_results) > 0) {
  # Use k with best performance if we have results
  best_k_name <- names(performance_results)[1]  # Default to first one that worked
  cat("Using k =", best_k_name, "for final model\n")
  best_k <- as.numeric(best_k_name)
}

# final train/test split
set.seed(123)
which_train <- sample(x = c(TRUE, FALSE),
                     size = nrow(filtered_ratings),
                     replace = TRUE,
                     prob = c(0.8, 0.2))
train_data <- filtered_ratings[which_train, ]
test_data <- filtered_ratings[!which_train, ]

# building the final model with best k parameter
final_model <- Recommender(train_data, method = "IBCF", parameter = list(k = best_k))

# make predictions, recommendations
n_recommended <- 5
predicted <- predict(final_model, newdata = test_data, n = n_recommended)

# convert to list & assign correct names
recommendation_list <- as(predicted, "list")
names(recommendation_list) <- rownames(test_data)

# show first 3 users' recommendations
recommendation_list[1:3]

# Simple evaluation - just see what % of recommendations are useful (precision-like metric)
cat("Final model using k =", best_k, "\n")
```

```{This is the older one, needs to be decided on which to go with.}

{r}
data_rcs <- data

# Load packages
library(dplyr)
library(tidyr)
library(recommenderlab)

# Step 1: Create binary interaction dataset (purchases)
interactions <- data %>%
  dplyr::select(customer_id, product_title) %>%
  distinct() %>%
  mutate(purchased = 1)

# Step 2: Sample a subset of customers
set.seed(123)
sample_customers <- sample(unique(interactions$customer_id), 1000)

sample_data <- interactions %>%
  filter(customer_id %in% sample_customers)

# Step 3: Filter to top 100 most purchased products
top_products <- sample_data %>%
  dplyr::count(product_title, sort = TRUE) %>%
  slice_head(n = 100) %>%
  pull(product_title)

sample_data <- sample_data %>%
  filter(product_title %in% top_products)

# Step 4: Create wide user-product matrix (1 = purchased)
interaction_matrix <- sample_data %>%
  tidyr::pivot_wider(
    names_from = product_title,
    values_from = purchased,
    values_fill = list(purchased = 0)
  )

# Step 5: Convert to matrix format
customer_ids <- interaction_matrix$customer_id
interaction_matrix <- interaction_matrix %>% dplyr::select(-customer_id)
mat <- as.matrix(interaction_matrix)
rownames(mat) <- customer_ids

# Step 6: Convert to binaryRatingMatrix
binary_ratings <- as(mat, "binaryRatingMatrix")


# remove sparse customers/products
# Keep customers with more 
filtered_ratings <- binary_ratings[rowCounts(binary_ratings) > 2, colCounts(binary_ratings) > 5]

# train/test split
set.seed(42)
which_train <- sample(x = c(TRUE, FALSE),
                      size = nrow(filtered_ratings),
                      replace = TRUE,
                      prob = c(0.8, 0.2))

train_data <- filtered_ratings[which_train, ]
test_data <- filtered_ratings[!which_train, ]

# Restore customer IDs for test set
rownames(test_data) <- rownames(filtered_ratings)[!which_train]

# Build model
recommender_model <- Recommender(train_data, method = "IBCF", parameter = list(k = 30))

# Predict
n_recommended <- 5
predicted <- predict(recommender_model, newdata = test_data, n = n_recommended)

# Convert to list & assign correct names
recommendation_list <- as(predicted, "list")
names(recommendation_list) <- rownames(test_data)


# Show first 3 users’ recommendations
recommendation_list[1:3]

# evaluate model performance
scheme <- evaluationScheme(filtered_ratings, method = "split", train = 0.8, given = -1)

results <- evaluate(scheme, method = "IBCF", n = c(1, 3, 5, 10))

avg(results)
```

```{r}
# Filter RFM to match recommendation users
rfm_sample <- segment %>%
  filter(customer_id %in% names(recommendation_list))

# Create recommendation data frame
recommendation_df <- data.frame(
  customer_id = names(recommendation_list),
  recommended_items = sapply(recommendation_list, paste, collapse = ", ")
)

# Match data types
recommendation_df$customer_id <- as.character(recommendation_df$customer_id)
rfm_sample$customer_id <- as.character(rfm_sample$customer_id)

# Join recommendations with RFM data
final_output <- recommendation_df %>%
  dplyr::left_join(rfm_sample, by = "customer_id")

print(table(final_output$segment))

view(final_output)




```
