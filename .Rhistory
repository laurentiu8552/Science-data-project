<<<<<<< HEAD
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 100))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
library(dplyr)
library(lubridate)
#getting the clv data neccesary
clv_data <- data %>%
group_by(customer_id) %>%
dplyr::summarise(
sales = revenue,
date = date
) %>%
ungroup()
# Getting the helper collumn: first time purchase
clv_data <- clv_data %>%
arrange(customer_id, date) %>%  # Ensure data is sorted by date
group_by(customer_id) %>%
mutate(first_purchase_date = min(date)) %>%  # Get the first purchase date per customer
ungroup()
#Removing the helper collumns and filtering so no newcomers in the test set
clv_data <- clv_data %>%
dplyr::filter(first_purchase_date < as.Date("2024-09-01")) %>%
dplyr::select(-first_purchase_date)
clv_data <- as.data.frame(clv_data)
# Remove customer with ID 9999999999
clv_data <- clv_data %>%
filter(customer_id != 9999999999)
#################################
#TESTING ON A SMALLER DATASET DELETE LATER !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
# clv_data <- clv_data %>%
#   sample_frac(0.1)  # Select 10%
#################################
# Select subset
clv_subset <- mutate(clv_data[,c(1,2,3)], date =as.Date(date))
names(clv_subset) <- c("cust", "sales", "date")
length(unique(clv_subset$cust))
data_btyd <- dc.MergeTransactionsOnSameDate(clv_subset)
#### BTYD prepping data final
end.of.cal.period <- as.Date("2024-09-01")
data_btyd_recency <- dc.ElogToCbsCbt(data_btyd, per="day", T.cal = end.of.cal.period,  statistic = "freq")
cal.cbs <- as.matrix(data_btyd_recency[[1]][[1]])
variable.names(cal.cbs)
#### --- Model prediction ---
#____________
#### Pareto/NBD
## Model estimation
# ## Estimate the parameters of the Pareto/NBD model using the calibration matrix
# params.pareto <- BTYD::pnbd.EstimateParameters(cal.cbs = cal.cbs)
# params.pareto
#
# # Calculate and print the log likelihood of the fitted Pareto/NBD model
# LL.pareto <- pnbd.cbs.LL(params.pareto, data_btyd_recency$cal$cbs)
# LL.pareto
#
# ## Plots ##
#   # Plot the fit of the model during calibration period - aggregated data
#   pnbd.PlotFrequencyInCalibration(params.pareto, data_btyd_recency$cal$cbs, censor = 7)
#_________________
#### BG/NBD
## Model estimation ##
# Estimate parameters for the BG/NBD model using the calibration matrix
params.bg <- bgnbd.EstimateParameters(cal.cbs, max.param.value = 10000)
params.bg  # Print the estimated parameters
# Log likelihood of the model
LL.bg <- bgnbd.cbs.LL(params.bg, cal.cbs)
LL.bg
## Plots ##
# Calibration period fit - aggregated plots
bgnbd.PlotFrequencyInCalibration(params.bg, cal.cbs, 7)
#### Gamma-gamma monetary model
# Create data
cal.cbs1 <- as.data.frame(cal.cbs) # Convert calibration matrix to a data frame
cal.cbs1 <- tibble::rownames_to_column(cal.cbs1, "cust") # Add row names as a customer ID column
cal.cbs1$cust <- as.integer(cal.cbs1$cust) # Convert customer IDs to integer type
data_gamma_train<- subset(clv_subset, date < as.Date("2024-09-01")) # Subset the original data to include only transactions before a specified date
df_gamma_train <- data_gamma_train %>% group_by(cust) %>% summarise_at(vars(sales), list(m.x = mean)) # Group data by customer and calculate the mean sales per
cal.cbs.gamma <- merge(cal.cbs1, df_gamma_train, by = "cust") # Merge calibration data with transaction means
# hence, m.x is the average spending per transaction for each customer
# Obtain parameters
ave.spend <- cal.cbs.gamma$m.x # Extract the average spending per transaction for each customer
tot.trans <- cal.cbs.gamma$x # Extract the total number of transactions for each customer
ave.spend <- ave.spend[which(tot.trans >0)] # Filter for customers with more than 0 transactions
tot.trans <- tot.trans[which(tot.trans >0)] # Filter for customers with more than 0 transactions
params_gamma <- spend.EstimateParameters(m.x.vector = ave.spend, x.vector = tot.trans) # Estimate the Gamma-Gamma model parameters
params_gamma
# Plot
spend.plot.average.transaction.value(
params = params_gamma,
m.x.vector = ave.spend,
x.vector = tot.trans,
xlab = "Average Transaction Value",
ylab = "Marginal Distribution of Average Transaction Value",
title = "Actual vs. Expected Average Transaction Value Across Customers"
)
library(ggpubr)
# Evaluate independence assumption
# We aim to check correlation coefficient between average spend (ave.spend)
# and the number of repeat purchases (tot.trans.trunc)
ave.spend #average transaction value in training data
tot.trans.trunc <- tot.trans
tot.trans.trunc[tot.trans.trunc > 7] = 7   # Cap the transactions at 7 to limit extreme values
data.ass <- data.frame(tot.trans.trunc, ave.spend) # Create a new data frame for assessing independence
# Plot a boxplot to visually evaluate the independence of average spend and number of transactions
ggboxplot(data.ass, x = "tot.trans.trunc", y = "ave.spend",
fill = "grey 88", ylab = "Average purchase value", xlab = "Number of repeat purchases") +
theme(axis.text=element_text(size=14, face = "plain"),
axis.title=element_text(size=15, face = "plain"))
# Calculate the correlation between average spend and number of transactions to evaluate the independence assumption
cor(data.ass$ave.spend, data.ass$tot.trans.trunc)
# Real and predicted CLV values (1-year predictions)
# ________________________________________________
# Pareto/NBD
# Predict the expected number of transactions for the next year using
# Pareto/NBD model parameters
# Given the past behavior captured in the calibration period and the model
# parameters derived from it, it estimates the number of transactions a
# customer will make in the next T.star period.
#___________________
#  library(BTYD)
# T.star <- max(data_btyd_recency$holdout$cbs[,"T.star"])
#
# cal.cbs1$yearpred.pareto <- round(pnbd.ConditionalExpectedTransactions(
# params.pareto,
# T.star = T.star, # Duration of the prediction period (holdout sample)
# x = cal.cbs1$x, # Number of repeat transactions
# t.x = cal.cbs1$t.x, # Time of last transaction
# T.cal = cal.cbs1$T.cal, # Length of the calibration period
# hardie = TRUE),2)
#_________________
# using BG/NBD -- same idea
T.star <- max(data_btyd_recency$holdout$cbs[,"T.star"])
cal.cbs1$yearpred.bg <- round(bgnbd.ConditionalExpectedTransactions(
params.bg,
T.star = T.star,
x = cal.cbs1$x,
t.x = cal.cbs1$t.x,
T.cal = cal.cbs1$T.cal,
hardie = TRUE),2)
#________
# Gamma-Gamma for expected monetary value
# Calculate the expected average monetary value per transaction
# using the Gamma-Gamma parameters
cal.cbs.gamma$spend <- spend.expected.value(
params = params_gamma,
m.x = cal.cbs.gamma$m.x, # the mean monetary value per transaction for each customer during the calibration period
x =cal.cbs.gamma$x) # the number of transactions each customer made during the calibration period
# Calculate average real sale value from validation data starting from a specific date
data_gamma_test <- subset(data_btyd, date >= as.Date("2024-09-01"))
df_gamma_test <- data_gamma_test %>%
group_by(cust) %>%
summarise_at(vars(sales),
list(m.x.avg.real.sale.test = mean))
cal.cbs.gamma.test <- merge(cal.cbs.gamma, df_gamma_test, by = "cust", all.x = TRUE)
# Count real total purchases for each customer in validation data and added too
Freq <- as.data.frame(table(data_gamma_test$cust))
cal.cbs.gamma.test <- merge(cal.cbs.gamma.test, Freq, by.x = "cust", by.y = "Var1", all.x = TRUE)
# Replace NA with 0 for customers who do not have repeat purchases
cal.cbs.gamma.test[is.na(cal.cbs.gamma.test)] = 0
# Integrate the predictions from probability models too
cal.cbs.gamma.test <- merge(cal.cbs.gamma.test, cal.cbs1, by.x = "cust", by.y = "cust", all.x = TRUE)
cal.cbs.gamma.test
# Calculate expected CLV (1-year predictions, validation period)
# by multiplying expected transactions by the expected monetary value
#_____
# cal.cbs.gamma.test$CLV.pred.pareto <- cal.cbs.gamma.test$spend * cal.cbs.gamma.test$yearpred.pareto # Pareto/NBD
#_____
cal.cbs.gamma.test$CLV.pred.bg <- cal.cbs.gamma.test$spend * cal.cbs.gamma.test$yearpred.bg # BG/NBD
# Calculate real CLV based on actual sales and transaction frequency
# Multiply real transactions by the monetary value
cal.cbs.gamma.test$CLV.real <- cal.cbs.gamma.test$m.x.avg.real.sale.test * cal.cbs.gamma.test$Freq
head(cal.cbs.gamma.test[, c(13:14)]) #to be added later
# CLV.pred.pareto CLV.pred.bg CLV.real
# 1      0.000000    0.000000      0.0
# 2      2.907413    8.722238      0.0
# 3     883.167227  692.946593    481.3
# 4     10.333488   48.222945     0.0
# 5     93.703514   95.540838     0.0
# 6     3.444496   24.111473      0.0
# Performance measures: expected CLV vs. real CLV
# Individual level
#_____________
# Pareto/NBD
# rmse_pareto <- rmse(cal.cbs.gamma.test$CLV.real, cal.cbs.gamma.test$CLV.pred.pareto)
# mae_pareto <- mae(cal.cbs.gamma.test$CLV.real, cal.cbs.gamma.test$CLV.pred.pareto)
# _____________
#BG/NBD
rmse_bg <- rmse(cal.cbs.gamma.test$CLV.real, cal.cbs.gamma.test$CLV.pred.bg)
mae_bg <- mae(cal.cbs.gamma.test$CLV.real, cal.cbs.gamma.test$CLV.pred.bg)
#### Summary of evaluation (MSE and RMSE)
models_rmse = cbind(rmse_bg)
models_mae = cbind(mae_bg)
models_stats = rbind(models_rmse, models_mae)
rownames(models_stats) = c("RMSE", "MAE")
colnames(models_stats) = c("BG/NBD")
round(models_stats,3)
#         Pareto/NBD   BG/NBD
# RMSE   2667.122     2672.224
# MAE     971.698     955.218
# Create a histogram of CLV.real
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 100))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
# Create a histogram of CLV.real
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 1000))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 7000))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 70000))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 1000))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 70000))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 700000))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 100000))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
# Create a histogram of CLV.real
hist(cal.cbs.gamma.test$CLV.real,
main = "Histogram of CLV.real",
xlab = "CLV Value",
col = rgb(0, 0, 1, 0.5),  # Blue with transparency for real values
border = "black",
breaks = 50,
xlim = c(0, 7000),  # Set the x-axis range to focus on 0-7000
ylim = c(0, 90000))  # Adjust y-axis range for better visualization
# Add a legend
legend("topright",
legend = c("CLV.real"),
fill = c(rgb(0, 0, 1, 0.5)))
#data3 - clv_subset
#data1 - data_btyd
#data2 - data_btyd_recency
View(cal.cbs.gamma.test)
summary(cal.cbs.gamma.test)
library(ggplot2)
ggplot(data, aes(x = CLV.pred.bg, y = CLV.real)) +
geom_point(alpha = 0.5, color = "blue") +
geom_smooth(method = "lm", color = "red", se = FALSE) +
labs(title = "Predicted vs Real CLV",
x = "Predicted CLV (CLV.pred.bg)",
y = "Real CLV (CLV.real)") +
theme_minimal()
library(ggplot2)
ggplot(cal.cbs.gamma.test, aes(x = CLV.pred.bg, y = CLV.real)) +
geom_point(alpha = 0.5, color = "blue") +
geom_smooth(method = "lm", color = "red", se = FALSE) +
labs(title = "Predicted vs Real CLV",
x = "Predicted CLV (CLV.pred.bg)",
y = "Real CLV (CLV.real)") +
theme_minimal()
library(ggplot2)
ggplot(cal.cbs.gamma.test, aes(x = CLV.pred.bg, y = CLV.real)) +
geom_point(aes(color = "Data Points"), alpha = 0.5) +  # Add color mapping for legend
geom_smooth(aes(color = "Trend Line"), method = "lm", se = FALSE) +
labs(title = "Predicted vs Real CLV",
x = "Predicted CLV (CLV.pred.bg)",
y = "Real CLV (CLV.real)",
color = "Legend") +  # Add legend title
scale_color_manual(values = c("Data Points" = "blue", "Trend Line" = "red")) +
theme_minimal()
View(data)
write.csv(data, "data.csv", row.names = FALSE)
library(ggplot2)
ggplot(cal.cbs.gamma.test, aes(x = CLV.pred.bg, y = CLV.real)) +
geom_point(aes(color = "Data Points"), alpha = 0.5) +  # Add color mapping for legend
geom_smooth(aes(color = "Trend Line"), method = "lm", se = FALSE) +
labs(title = "Predicted vs Real CLV",
x = "Predicted CLV (CLV.pred.bg)",
y = "Real CLV (CLV.real)",
color = "Legend") +  # Add legend title
scale_color_manual(values = c("Data Points" = "blue", "Trend Line" = "red")) +
theme_minimal()
#### BG/NBD
## Model estimation ##
# Estimate parameters for the BG/NBD model using the calibration matrix
params.bg <- bgnbd.EstimateParameters(cal.cbs, max.param.value = 10000)
params.bg  # Print the estimated parameters
# Log likelihood of the model
LL.bg <- bgnbd.cbs.LL(params.bg, cal.cbs)
LL.bg
## Plots ##
# Calibration period fit - aggregated plots
bgnbd.PlotFrequencyInCalibration(params.bg, cal.cbs, 7)
# Evaluate independence assumption
# We aim to check correlation coefficient between average spend (ave.spend)
# and the number of repeat purchases (tot.trans.trunc)
ave.spend #average transaction value in training data
tot.trans.trunc <- tot.trans
tot.trans.trunc[tot.trans.trunc > 7] = 7   # Cap the transactions at 7 to limit extreme values
data.ass <- data.frame(tot.trans.trunc, ave.spend) # Create a new data frame for assessing independence
# Plot a boxplot to visually evaluate the independence of average spend and number of transactions
ggboxplot(data.ass, x = "tot.trans.trunc", y = "ave.spend",
fill = "grey 88", ylab = "Average purchase value", xlab = "Number of repeat purchases") +
theme(axis.text=element_text(size=14, face = "plain"),
axis.title=element_text(size=15, face = "plain"))
# Calculate the correlation between average spend and number of transactions to evaluate the independence assumption
cor(data.ass$ave.spend, data.ass$tot.trans.trunc)
# BG/NBD
T.star <- max(data_btyd_recency$holdout$cbs[,"T.star"])
cal.cbs1$yearpred.bg <- round(bgnbd.ConditionalExpectedTransactions(
params.bg,
T.star = T.star,
x = cal.cbs1$x,
t.x = cal.cbs1$t.x,
T.cal = cal.cbs1$T.cal,
hardie = TRUE),2)
cal.cbs.gamma$spend <- spend.expected.value(
params = params_gamma,
m.x = cal.cbs.gamma$m.x,
x =cal.cbs.gamma$x)
data_gamma_test <- subset(data_btyd, date >= as.Date("2024-09-01"))
df_gamma_test <- data_gamma_test %>%
group_by(cust) %>%
summarise_at(vars(sales),
list(m.x.avg.real.sale.test = mean))
cal.cbs.gamma.test <- merge(cal.cbs.gamma, df_gamma_test, by = "cust", all.x = TRUE)
Freq <- as.data.frame(table(data_gamma_test$cust))
cal.cbs.gamma.test <- merge(cal.cbs.gamma.test, Freq, by.x = "cust", by.y = "Var1", all.x = TRUE)
cal.cbs.gamma.test[is.na(cal.cbs.gamma.test)] = 0
cal.cbs.gamma.test <- merge(cal.cbs.gamma.test, cal.cbs1, by.x = "cust", by.y = "cust", all.x = TRUE)
cal.cbs.gamma.test
cal.cbs.gamma.test$CLV.pred.bg <- cal.cbs.gamma.test$spend * cal.cbs.gamma.test$yearpred.bg # BG/NBD
cal.cbs.gamma.test$CLV.real <- cal.cbs.gamma.test$m.x.avg.real.sale.test * cal.cbs.gamma.test$Freq
ggplot(cal.cbs.gamma.test, aes(x = CLV.pred.bg, y = CLV.real)) +
geom_point(aes(color = "Data Points"), alpha = 0.5) +
geom_smooth(aes(color = "Trend Line"), method = "lm", se = FALSE) +
labs(title = "Predicted vs Real CLV",
x = "Predicted CLV (CLV.pred.bg)",
y = "Real CLV (CLV.real)",
color = "Legend") +
scale_color_manual(values = c("Data Points" = "blue", "Trend Line" = "red")) +
theme_minimal()
=======
"Bed Linen",
"Bed Linen",
"Bed Linen",
"Bed Linen",
"Mattresses",
"Bed Linen",
"Mattresses",
"Bed Linen",
"Bed Linen",
"Mattresses",
"Bed Linen",
"Mattresses",
"Mattresses",
"Mattresses",
"Mattresses",
"Furniture",
"Furniture",
"Textiles",
"Bathroom",
"Bed Linen",
"Mattresses",
"Mattresses",
"Mattresses",
"Bed Linen",
"Bed Linen",
"Mattresses",
"Mattresses",
"Furniture",
"Mattresses",
"Mattresses",
"Mattresses",
"Furniture",
"Furniture",
"Textiles",
"Mattresses",
"Bed Linen",
"Furniture",
"Homeware",
"Garden",
"Bed Linen",
"Bed Linen",
"Mattresses",
"Mattresses",
"Mattresses",
"Textiles",
"Mattresses",
"Mattresses",
"Bed Linen",
"Mattresses",
"Furniture",
"Furniture",
"Furniture",
"Furniture",
"Mattresses",
"Mattresses",
"Mattresses",
"Bed Linen",
"Mattresses",
"Mattresses",
"Mattresses",
"Mattresses",
"Mattresses",
"Bathroom",
"Textiles",
"Mattresses",
"Mattresses",
"Furniture",
"Furniture",
"Mattresses",
"Mattresses",
"Mattresses",
"Mattresses",
"Mattresses"
)
# c, define the 'product_category_level_2' vector --------
product_category_level_2 <- c(
"mirror",
"Mattresses",
"sofa bed",
"bed frame",
"bed frame",
"Towel",
"Mattresses",
"top mattress",
"Mattresses",
"Mattresses",
"Mattresses",
"Mattresses",
"dining chair",
"Mattresses",
"Mattresses",
"Mattresses",
"Bed Linen",
"pouffe",
"Bed Linen",
"Towel",
"Mattresses",
"desk",
"Towel",
"Mattresses",
"bed frame",
"Towel",
"Sheets",
"Mattresses",
"Towel",
"footstool",
"bed frame",
"Mattresses",
"Mattresses",
"shelving unit",
"Mattresses",
"Mattresses",
"Mattresses",
"Curtain",
"Sheets",
"Mattresses",
"Towel",
"Sheets",
"Mattresses",
"top mattress",
"bed frame",
"TableTextile",
"bed frame",
"wardrobe",
"bed frame",
"Mattresses",
"top mattress",
"Mattresses",
"Mattresses",
"top mattress",
"Mattresses",
"Sheets",
"Sheets",
"sofa bed",
"Mattresses",
"stool",
"Mattresses",
"Towel",
"TableTextile",
"Sheets",
"Sheets",
"Pillow",
"Mattresses",
"Decoration",
"pouffe",
"shelving unit",
"Mattresses",
"Towel",
"Sheets",
"Mattresses",
"top mattress",
"Sheets",
"Sheets",
"Bed Linen",
"Mattresses",
"Sheets",
"Towel",
"Accessorries",
"nest of tables",
"continental bed",
"Mattresses",
"Mattresses",
"chest of drawers",
"Towel",
"continental bed",
"Mattresses",
"Mattresses",
"Mattresses",
"Bed Linen",
"Bed Linen",
"Decoration",
"Sheets",
"sofa bed",
"desk",
"Towel",
"Sheets",
"Sheets",
"top mattress",
"Mattresses",
"bed frame",
"pouffe",
"Mattresses",
"Mattresses",
"bench",
"Bed Linen",
"Mattresses",
"bed frame",
"Sheets",
"pouffe",
"Sheets",
"Pillow",
"Mattresses",
"Mattresses",
"Mattresses",
"sofa bed",
"Sheets",
"Mattresses",
"Garden table",
"Curtain",
"armchair",
"Mattresses",
"top mattress",
"Sheets",
"Decoration",
"Sheets",
"Sheets",
"Bed Linen",
"Bed Linen",
"Sheets",
"top mattress",
"Sheets",
"Mattresses",
"Bed Linen",
"Bed Linen",
"continental bed",
"Bed Linen",
"Mattresses",
"Mattresses",
"Mattresses",
"Mattresses",
"chest of drawers",
"pedestal",
"Curtain",
"Towel",
"Bed Linen",
"Accessorries",
"Mattresses",
"Accessorries",
"Sheets",
"Bed Linen",
"Accessorries",
"Mattresses",
"shelving unit",
"continental bed",
"Accessorries",
"Mattresses",
"decoration",
"nest of tables",
"Curtain",
"Mattresses",
"Bed Linen",
"bedside table",
"lamp",
"Garden cushion",
"Bed Linen",
"Bed Linen",
"top mattress",
"Mattresses",
"Accessorries",
"Curtain",
"Accessorries",
"continental bed",
"Bed Linen",
"Mattresses",
"office chair",
"sofa bed with chaise longue",
"dining chair",
"dining chair",
"Accessorries",
"Accessorries",
"Accessorries",
"Bed Linen",
"top mattress",
"Accessorries",
"top mattress",
"Mattresses",
"top mattress",
"Towel",
"TableTextile",
"Mattresses",
"Accessorries",
"sofa bed",
"Decoration",
"Accessorries",
"top mattress",
"Mattresses",
"Mattresses",
"Mattresses"
)
# d, combine all three vectors into a data frame named 'lookup'
lookup <- data.frame(
product_title = product_title,
product_group_level_1 = product_group_level_1,
product_category_level_2 = product_category_level_2,
stringsAsFactors = FALSE
)
# e, Print to confirm
print(lookup, n = nrow(lookup))
# f, now the actual imputation part
# create data_joined by joining data with lookup on product_title
data <- data %>%
left_join(lookup, by = "product_title", suffix = c("", ".lkp")) %>%
# replace NAs in original columns with the lookup columns
mutate(
product_group_level_1 = coalesce(product_group_level_1, product_group_level_1.lkp),
product_category_level_2 = coalesce(product_category_level_2, product_category_level_2.lkp  )
) %>%
# remove the extra lookup columns
dplyr::select(-ends_with(".lkp"))
# Convert the 'date' column to Date type
data$date <- as.Date(data$date, format = "%d.%m.%Y")
# Convert all non-numeric columns (besides 'date') to factors
data[] <- lapply(data, function(x) if(is.character(x)) as.factor(x) else x)
# Impute NA customer id's with 999999999 for now. Will be removed in CLV probably
data <- data %>%
mutate(customer_id = replace_na(customer_id, 9999999999))
sum(is.na(data))
skimr::skim(data)
#Visualization of the 3 rfm variables
# Revenue histogram
hist(data_clean_rfm$revenue, main = "Histogram of Revenue", col = "blue", border = "black")
>>>>>>> d6189d14d8172d8d8e1f98da4852d36c492f9300
#Visualization of the 3 rfm variables
data_clean_rfm = data
# Revenue histogram
hist(data_clean_rfm$revenue, main = "Histogram of Revenue", col = "blue", border = "black")
# Check for NAs
sum(is.na(data_clean_rfm$revenue))      # Count NAs in revenue
# Customer ID bar plot - Delete later, useless, including the NA checks
cust_freq <- table(data_clean_rfm$customer_id[data_clean_rfm$customer_id != 9999999999])
# Filter to only those with frequency >= 10
cust_freq_filtered <- cust_freq[cust_freq >= 35]
barplot(cust_freq_filtered, main = "Customer ID Distribution", col = "blue", las = 2, ylim = c(0, 80))
# Date bar plot
barplot(table(data_clean_rfm$date), main = "Date Distribution", col = "blue")
sum(is.na(data_clean_rfm$date))         # Count NAs in date
#CAN BE SEEN no NAs and also no negative variables, can proceed with the RFM model running
<<<<<<< HEAD
#APPLICATION 2
#starting with descriptive statistics
#Cleaning the data on na customers but also revenue above 0
data_application2 <- data %>%
filter(!is.na(customer_id))
library(tidyverse)
library(caret)
library(caTools)
library(DataExplorer)
library(ggplot2)
library(visdat)
library(recipes)
library(rsample)
library(forecast)
library(MASS)
library(rfm)
library(readxl)
library(dplyr)
library(ggplot2)
library(DataExplorer)
library(GGally)
library(BTYD)
library(plyr)
library(ggplot2)
library(lubridate)
library(reshape2)
library(BTYDplus)
library(ggpubr)
library(Hmisc)
library(tibble)
library(Metrics)
library(magrittr)
library(dplyr)
library(car)
library(rfm)
library(kableExtra)
library(reshape2)
library(tidyverse)
library(knitr)
library(flexclust)
library(clue)
library(openxlsx)
library(tidygeocoder)
library(sf)
=======
#RFM model
analysis_date = as.Date("2024-12-31")
# Run the RFM analysis
rfm_result <- rfm_table_order(data_clean_rfm,
customer_id,
date,
revenue,
analysis_date = analysis_date)
# Step 3: Visualize RFM Results
# Heatmap visualization of RFM scores
rfm_plot_heatmap(rfm_result)
# Bar chart of RFM scores
rfm_bar_chart(rfm_result)
# Step 4: Segment Customers based on RFM scores
# Define RFM segment categories
segment_names <- c("Champions", "Loyal Customers", "Potential Loyalist",
"New Customers", "Promising", "Need Attention", "About To Sleep",
"At Risk", "Can't Lose Them", "Lost")
# Set upper and lower bounds for Recency, Frequency, and Monetary
recency_lower   <- c(4, 2, 3, 4, 3, 2, 2, 1, 1, 1)
recency_upper   <- c(5, 5, 5, 5, 4, 3, 3, 2, 1, 2)
frequency_lower <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
frequency_upper <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
monetary_lower  <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
monetary_upper  <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
# Apply segments based on RFM scores
segment <- rfm_segment(rfm_result,
segment_names,
recency_lower, recency_upper,
frequency_lower, frequency_upper,
monetary_lower, monetary_upper)
# View the segmented customers
head(segment)
# Step 5: Visualize RFM Segments
# Recency by segment
rfm_plot_median_recency(segment, sort = TRUE)
# Monetary by segment
rfm_plot_median_monetary(segment, sort = TRUE)
# Frequency by segment
rfm_plot_median_frequency(segment, sort = TRUE)
#Visualization of the 3 rfm variables
data_clean_rfm = data
data_clean_rfm = data_clean_rfm$customer_id != 9999999999
# Revenue histogram
hist(data_clean_rfm$revenue, main = "Histogram of Revenue", col = "blue", border = "black")
#Visualization of the 3 rfm variables
data_clean_rfm <- data[data$customer_id != 9999999999, ]
# Revenue histogram
hist(data_clean_rfm$revenue, main = "Histogram of Revenue", col = "blue", border = "black")
# Check for NAs
sum(is.na(data_clean_rfm$revenue))      # Count NAs in revenue
# Customer ID bar plot - Delete later, useless, including the NA checks
cust_freq <- table(data_clean_rfm$customer_id)
# Filter to only those with frequency >= 10
cust_freq_filtered <- cust_freq[cust_freq >= 35]
barplot(cust_freq_filtered, main = "Customer ID Distribution", col = "blue", las = 2, ylim = c(0, 80))
# Date bar plot
barplot(table(data_clean_rfm$date), main = "Date Distribution", col = "blue")
sum(is.na(data_clean_rfm$date))         # Count NAs in date
#CAN BE SEEN no NAs and also no negative variables, can proceed with the RFM model running
#RFM model
analysis_date = as.Date("2024-12-31")
# Run the RFM analysis
rfm_result <- rfm_table_order(data_clean_rfm,
customer_id,
date,
revenue,
analysis_date = analysis_date)
# Step 3: Visualize RFM Results
# Heatmap visualization of RFM scores
rfm_plot_heatmap(rfm_result)
# Bar chart of RFM scores
rfm_bar_chart(rfm_result)
# Step 4: Segment Customers based on RFM scores
# Define RFM segment categories
segment_names <- c("Champions", "Loyal Customers", "Potential Loyalist",
"New Customers", "Promising", "Need Attention", "About To Sleep",
"At Risk", "Can't Lose Them", "Lost")
# Set upper and lower bounds for Recency, Frequency, and Monetary
recency_lower   <- c(4, 2, 3, 4, 3, 2, 2, 1, 1, 1)
recency_upper   <- c(5, 5, 5, 5, 4, 3, 3, 2, 1, 2)
frequency_lower <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
frequency_upper <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
monetary_lower  <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
monetary_upper  <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
# Apply segments based on RFM scores
segment <- rfm_segment(rfm_result,
segment_names,
recency_lower, recency_upper,
frequency_lower, frequency_upper,
monetary_lower, monetary_upper)
# View the segmented customers
head(segment)
# Step 5: Visualize RFM Segments
# Recency by segment
rfm_plot_median_recency(segment, sort = TRUE)
# Monetary by segment
rfm_plot_median_monetary(segment, sort = TRUE)
# Frequency by segment
rfm_plot_median_frequency(segment, sort = TRUE)
table(rfm_result$frequency_score)
View(rfm_result)
View(rfm_result)
table(segment$segment)
View(rfm_result)
table(rfm_result$rfm$frequency_score)
View(rfm_result)
summary(rfm_result$rfm$monetary_score)
summary(rfm_result$rfm$recency_score)
summary(rfm_result$rfm$frequency_score)
hist(rfm_result$rfm$recency_score)
hist(rfm_result$rfm$frequency_score)
hist(rfm_result$rfm$monetary_score)
#RFM model
analysis_date = as.Date("2024-12-31")
# Run the RFM analysis
rfm_result <- rfm_table_order(data_clean_rfm,
customer_id,
date,
revenue,
analysis_date = analysis_date)
# Step 3: Visualize RFM Results
# Heatmap visualization of RFM scores
rfm_plot_heatmap(rfm_result)
# Bar chart of RFM scores
rfm_bar_chart(rfm_result)
# Step 4: Segment Customers based on RFM scores
# Define RFM segment categories
segment_names <- c("Champions", "Loyal Customers", "Potential Loyalist",
"New Customers", "Promising", "Need Attention", "About To Sleep",
"At Risk", "Can't Lose Them", "Lost")
# Set upper and lower bounds for Recency, Frequency, and Monetary
recency_lower   <- c(4, 2, 3, 4, 3, 2, 2, 1, 1, 1)
recency_upper   <- c(5, 5, 5, 5, 4, 3, 3, 2, 1, 2)
frequency_lower <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
frequency_upper <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
monetary_lower  <- c(4, 3, 1, 1, 1, 2, 1, 2, 4, 1)
monetary_upper  <- c(5, 5, 3, 1, 1, 3, 2, 5, 5, 2)
# Apply segments based on RFM scores
segment <- rfm_segment(rfm_result,
segment_names,
recency_lower, recency_upper,
frequency_lower, frequency_upper,
monetary_lower, monetary_upper)
# View the segmented customers
head(segment)
table(segment$segment)
table(rfm_result$rfm$frequency_score)
summary(rfm_result$rfm$monetary_score)
summary(rfm_result$rfm$recency_score)
summary(rfm_result$rfm$frequency_score)
hist(rfm_result$rfm$recency_score)
hist(rfm_result$rfm$frequency_score)
hist(rfm_result$rfm$monetary_score)
# Step 5: Visualize RFM Segments
# Recency by segment
rfm_plot_median_recency(segment, sort = TRUE)
# Monetary by segment
rfm_plot_median_monetary(segment, sort = TRUE)
# Frequency by segment
rfm_plot_median_frequency(segment, sort = TRUE)
summary(data_clean_rfm)
library(dplyr)
rfm_prepared <- data_clean_rfm %>%
select(date, order_id, customer_id, revenue) %>%  # keep only necessary columns
group_by(order_id, customer_id, date) %>%         # group by order and customer (date included if orders have dates)
summarise(revenue = sum(revenue, na.rm = TRUE)) %>% # sum revenue for each group
ungroup()
library(dplyr)
rfm_prepared <- data_clean_rfm %>%
dplyr::select(date, order_id, customer_id, revenue) %>%  # keep only necessary columns
group_by(order_id, customer_id, date) %>%         # group by order and customer (date included if orders have dates)
summarise(revenue = sum(revenue, na.rm = TRUE)) %>% # sum revenue for each group
ungroup()
View(rfm_prepared)
rfm_prepared <- data_clean_rfm %>%
dplyr::select(date, order_id, customer_id, revenue) %>%  # keep only necessary columns
group_by(order_id, customer_id, date) %>%         # group by order and customer (date included if orders have dates)
summarise(revenue = sum(revenue, na.rm = TRUE)) %>% # sum revenue for each group
View(rfm_prepared)
ungroup()
length(unique(data_clean_rfm$order_id))
length(unique(data_clean_rfm$customer_id))
length(unique(data_clean_rfm$date))
write.csv(data_clean_rfm)
write.csv(data_clean_rfm,"kungfufighting.csv")
>>>>>>> d6189d14d8172d8d8e1f98da4852d36c492f9300
